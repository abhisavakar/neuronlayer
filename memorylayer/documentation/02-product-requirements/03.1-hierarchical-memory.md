# Hierarchical Memory System

## Technical Specification

**Version**: 1.0  
**Status**: Draft  
**Target**: VS Code Extension (Phase 1)

---

## Overview

MemoryLayer implements a **three-tier hierarchical memory system** that mimics human memory architecture. This design enables efficient context management with O(1) retrieval complexity and constant token usage regardless of project size.

---

## Architecture Principles

### 1. Tiered Storage

Each tier serves a specific purpose with distinct performance characteristics:

```
Tier 1 (Working Context)          Tier 2 (Relevant Context)         Tier 3 (Archive)
┌──────────────────────┐          ┌──────────────────────┐         ┌──────────────────────┐
│ • Active file        │          │ • Semantic search    │         │ • Embeddings         │
│ • Recent decisions   │    →     │ • Related files      │    →    │ • Summaries          │
│ • Session state      │          │ • Dependencies       │         │ • Historical data    │
│ • ~1K tokens         │          │ • ~5K tokens         │         │ • Unlimited          │
│ Always loaded        │          │ Retrieved on-demand  │         │ Never sent raw       │
└──────────────────────┘          └──────────────────────┘         └──────────────────────┘
      Hot (RAM)                         Warm (SSD)                      Cold (Disk)
      <10ms access                      <100ms access                   <500ms access
```

### 2. Token Budget Management

**Total LLM Context Budget: 6,000 tokens**

```
┌─────────────────────────────────────────────────────────────┐
│                    6,000 Token Budget                       │
├─────────────────────────────────────────────────────────────┤
│ System Prompt:        500 tokens  (8%)                     │
│ Tier 1 (Working):   1,000 tokens  (17%)                    │
│ Tier 2 (Relevant):  4,000 tokens  (67%)                    │
│ Tier 3 (Retrieved):   500 tokens  (8%)                     │
│ Reserved/Overflow:      0 tokens  (0%)                     │
└─────────────────────────────────────────────────────────────┘
```

### 3. Access Patterns

**Tier 1**: Hot path—always in memory  
**Tier 2**: Warm path—cached with LRU eviction  
**Tier 3**: Cold path—demand-paged with predictive pre-fetch  

---

## Tier 1: Working Context

### Purpose
Ultra-fast access to current session state. Always loaded in memory.

### Contents

```typescript
interface WorkingContext {
  // Current file being edited
  activeFile: {
    path: string;
    content: string;           // Last 50 lines (or token limit)
    cursorPosition: { line: number; column: number };
    language: string;
    lastModified: Date;
  };

  // Recent decisions (last hour)
  recentDecisions: Decision[];

  // Active session summary
  sessionSummary: {
    startTime: Date;
    filesModified: string[];
    keyActions: string[];
    currentGoal?: string;
  };

  // Immediate context (last 10 minutes)
  recentContext: ContextSnippet[];
}

interface Decision {
  id: string;
  timestamp: Date;
  title: string;
  description: string;
  files: string[];
  tags: string[];
}
```

### Storage Implementation

**Format**: Plain text (JSON)  
**Location**: `~/.memorylayer/projects/{project-id}/tier1/working.json`  
**Update Frequency**: Real-time (on every file save)  
**Access Latency**: <10ms  

```typescript
// Storage engine
class Tier1Storage {
  private workingContext: WorkingContext;
  private projectId: string;

  async load(): Promise<WorkingContext> {
    // Load from disk if exists, otherwise initialize
    const path = this.getStoragePath();
    if (await fs.exists(path)) {
      const data = await fs.readFile(path, 'utf-8');
      return JSON.parse(data);
    }
    return this.initialize();
  }

  async save(context: WorkingContext): Promise<void> {
    // Atomic write to prevent corruption
    const tempPath = this.getStoragePath() + '.tmp';
    await fs.writeFile(tempPath, JSON.stringify(context, null, 2));
    await fs.rename(tempPath, this.getStoragePath());
  }

  async updateActiveFile(file: FileContext): Promise<void> {
    this.workingContext.activeFile = file;
    await this.save(this.workingContext);
  }

  async addDecision(decision: Decision): Promise<void> {
    // Keep only last 20 decisions
    this.workingContext.recentDecisions.unshift(decision);
    if (this.workingContext.recentDecisions.length > 20) {
      this.workingContext.recentDecisions = 
        this.workingContext.recentDecisions.slice(0, 20);
    }
    await this.save(this.workingContext);
  }
}
```

### Token Budget: 1,000 tokens

```
Active File Content:     ~600 tokens
Recent Decisions:        ~200 tokens (last 5)
Session Summary:         ~100 tokens
Recent Context:          ~100 tokens
─────────────────────────────────────
Total:                  1,000 tokens
```

---

## Tier 2: Relevant Context

### Purpose
Semantic search results and related code. Retrieved on-demand based on current task.

### Contents

```typescript
interface RelevantContext {
  // Semantic search results
  searchResults: SearchResult[];

  // Related files (dependency graph)
  relatedFiles: RelatedFile[];

  // Recent sessions (24h window)
  recentSessions: SessionSummary[];

  // Active decisions (not yet archived)
  activeDecisions: Decision[];

  // Code patterns and conventions
  patterns: CodePattern[];
}

interface SearchResult {
  file: string;
  content: string;
  relevance: number;        // 0-1 score
  contextType: 'function' | 'class' | 'interface' | 'variable';
  lineStart: number;
  lineEnd: number;
}

interface RelatedFile {
  path: string;
  relationship: 'imports' | 'exports' | 'shared' | 'test' | 'config';
  relevance: number;
  lastAccessed: Date;
}

interface CodePattern {
  pattern: string;
  frequency: number;
  examples: string[];
  description: string;
}
```

### Storage Implementation

**Format**: SQLite with vector extension (sqlite-vss)  
**Location**: `~/.memorylayer/projects/{project-id}/tier2/context.db`  
**Update Frequency**: On file change (debounced 5s)  
**Access Latency**: <100ms  

```typescript
// Database schema
const SCHEMA = `
  -- File embeddings for semantic search
  CREATE TABLE file_embeddings (
    id INTEGER PRIMARY KEY,
    file_path TEXT UNIQUE NOT NULL,
    content_hash TEXT NOT NULL,
    embedding BLOB NOT NULL,      -- 384-dim vector (all-MiniLM-L6-v2)
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
  );

  -- Dependency graph
  CREATE TABLE dependencies (
    id INTEGER PRIMARY KEY,
    source_file TEXT NOT NULL,
    target_file TEXT NOT NULL,
    relationship TEXT NOT NULL,
    strength REAL NOT NULL,
    UNIQUE(source_file, target_file)
  );

  -- Code patterns
  CREATE TABLE patterns (
    id INTEGER PRIMARY KEY,
    pattern_type TEXT NOT NULL,
    pattern_value TEXT NOT NULL,
    frequency INTEGER DEFAULT 1,
    examples TEXT  -- JSON array
  );

  -- Sessions
  CREATE TABLE sessions (
    id INTEGER PRIMARY KEY,
    session_id TEXT UNIQUE NOT NULL,
    start_time TIMESTAMP NOT NULL,
    end_time TIMESTAMP,
    summary TEXT,
    files_modified TEXT,  -- JSON array
    embedding BLOB        -- Session summary embedding
  );

  -- Vector search index
  CREATE VIRTUAL TABLE vss_files USING vss0(
    embedding(384)
  );
`;

// Storage engine
class Tier2Storage {
  private db: Database;
  private embeddingModel: EmbeddingModel;

  async initialize(): Promise<void> {
    this.db = await open({
      filename: this.getDatabasePath(),
      driver: sqlite3.Database
    });
    
    // Load vector extension
    await this.db.loadExtension('vector0');
    await this.db.loadExtension('vss0');
    
    // Create tables
    await this.db.exec(SCHEMA);
    
    // Load embedding model
    this.embeddingModel = await pipeline(
      'feature-extraction',
      'Xenova/all-MiniLM-L6-v2'
    );
  }

  async semanticSearch(
    query: string,
    limit: number = 10
  ): Promise<SearchResult[]> {
    // Generate query embedding
    const queryEmbedding = await this.generateEmbedding(query);
    
    // Vector similarity search
    const results = await this.db.all(`
      SELECT 
        f.file_path,
        f.content_preview,
        v.distance
      FROM vss_files v
      JOIN file_embeddings f ON v.rowid = f.id
      WHERE vss_search(v.embedding, ?)
      ORDER BY v.distance
      LIMIT ?
    `, [JSON.stringify(queryEmbedding), limit]);

    return results.map(r => ({
      file: r.file_path,
      content: r.content_preview,
      relevance: 1 - r.distance,  // Convert distance to relevance
      // ... additional fields
    }));
  }

  async indexFile(filePath: string, content: string): Promise<void> {
    const embedding = await this.generateEmbedding(content);
    const hash = crypto.createHash('sha256').update(content).digest('hex');
    
    await this.db.run(`
      INSERT OR REPLACE INTO file_embeddings 
      (file_path, content_hash, embedding)
      VALUES (?, ?, ?)
    `, [filePath, hash, Buffer.from(new Float32Array(embedding).buffer)]);
  }

  private async generateEmbedding(text: string): Promise<number[]> {
    const output = await this.embeddingModel(text, {
      pooling: 'mean',
      normalize: true
    });
    return Array.from(output.data);
  }
}
```

### Token Budget: 4,000 tokens

```
Semantic Search Results:   ~2,000 tokens (top 5 results)
Related Files:             ~1,200 tokens (key imports/exports)
Recent Sessions:             ~500 tokens (last 3 sessions)
Active Decisions:            ~300 tokens (recent decisions)
─────────────────────────────────────────────────────────
Total:                     4,000 tokens
```

---

## Tier 3: Archive

### Purpose
Long-term storage of project knowledge. Compressed embeddings and summaries.

### Contents

```typescript
interface ArchiveContext {
  // Historical sessions (compressed)
  sessionArchive: ArchivedSession[];

  // Architecture overview
  architecture: {
    components: Component[];
    dataFlow: DataFlow[];
    decisions: ArchitectureDecision[];
  };

  // Codebase summary
  codebaseSummary: {
    totalFiles: number;
    totalLines: number;
    languages: LanguageStats[];
    keyModules: string[];
    conventions: CodeConvention[];
  };

  // Project personality (learned patterns)
  personality: {
    codingStyle: string;
    preferredPatterns: string[];
    commonMistakes: string[];
    optimizationHints: string[];
  };
}

interface ArchivedSession {
  sessionId: string;
  dateRange: { start: Date; end: Date };
  summary: string;              // AI-generated summary
  keyDecisions: string[];
  embedding: number[];          // For retrieval
  compressed: boolean;
}

interface Component {
  name: string;
  type: 'service' | 'controller' | 'model' | 'utility';
  description: string;
  files: string[];
  dependencies: string[];
  embedding: number[];
}
```

### Storage Implementation

**Format**: SQLite + compressed JSON blobs  
**Location**: `~/.memorylayer/projects/{project-id}/tier3/archive.db`  
**Update Frequency**: Nightly (consolidation) + on-demand  
**Access Latency**: <500ms  

```typescript
// Archive storage with compression
class Tier3Storage {
  private db: Database;
  private compressionLevel: number = 9;  // zlib max compression

  async archiveSession(session: Session): Promise<void> {
    // Generate summary using local LLM
    const summary = await this.summarizeSession(session);
    
    // Create embedding
    const embedding = await this.generateEmbedding(summary);
    
    // Compress session data
    const compressed = await this.compressSession(session);
    
    await this.db.run(`
      INSERT INTO archived_sessions 
      (session_id, date_start, date_end, summary, embedding, compressed_data)
      VALUES (?, ?, ?, ?, ?, ?)
    `, [
      session.id,
      session.startTime,
      session.endTime,
      summary,
      Buffer.from(new Float32Array(embedding).buffer),
      compressed
    ]);
  }

  async retrieveRelevantArchives(
    query: string,
    limit: number = 3
  ): Promise<ArchivedSession[]> {
    const queryEmbedding = await this.generateEmbedding(query);
    
    // Vector search in archive
    const results = await this.db.all(`
      SELECT 
        session_id,
        summary,
        date_start,
        date_end,
        distance
      FROM vss_archives v
      JOIN archived_sessions a ON v.rowid = a.id
      WHERE vss_search(v.embedding, ?)
      ORDER BY v.distance
      LIMIT ?
    `, [JSON.stringify(queryEmbedding), limit]);

    return results.map(r => ({
      sessionId: r.session_id,
      dateRange: { start: r.date_start, end: r.date_end },
      summary: r.summary,
      relevance: 1 - r.distance
    }));
  }

  private async compressSession(session: Session): Promise<Buffer> {
    const json = JSON.stringify(session);
    return zlib.deflateSync(json, { level: this.compressionLevel });
  }

  private async summarizeSession(session: Session): Promise<string> {
    // Use local lightweight LLM for summarization
    const llm = await LocalLLM.load('Xenova/t5-small');
    const prompt = `Summarize this coding session in 2-3 sentences:\n${session.toString()}`;
    return await llm.generate(prompt, { maxLength: 100 });
  }
}
```

### Token Budget: 500 tokens (retrieved portion)

```
Archive Retrievals:        ~500 tokens (2-3 summaries)
──────────────────────────────────────────────────────
Total Retrieved:           500 tokens
Total Stored:              Unlimited (compressed)
```

---

## Context Assembly Pipeline

### Overview

The pipeline assembles context from all three tiers into the 6K token budget.

```
User Request
     ↓
Intent Detection (What are they trying to do?)
     ↓
Tier 1: Load Working Context (always)           [1,000 tokens]
     ↓
Tier 2: Retrieve Relevant Context (semantic)    [4,000 tokens]
     ↓
Tier 3: Retrieve Archive Fragments (predictive) [  500 tokens]
     ↓
Compression & Deduplication
     ↓
Final Context Package → LLM
```

### Implementation

```typescript
class ContextAssembler {
  private tier1: Tier1Storage;
  private tier2: Tier2Storage;
  private tier3: Tier3Storage;
  private tokenCounter: TokenCounter;

  async assembleContext(
    userIntent: Intent,
    currentFile: string
  ): Promise<ContextPackage> {
    const budget = new TokenBudget(6000);
    const context: ContextPackage = {
      system: await this.loadSystemPrompt(),
      working: null,
      relevant: [],
      archive: [],
      metadata: {
        totalTokens: 0,
        timestamp: new Date()
      }
    };

    // 1. Load Tier 1 (always included)
    context.working = await this.tier1.load();
    budget.allocate(1000, 'working');

    // 2. Retrieve Tier 2 (semantic search)
    const searchResults = await this.tier2.semanticSearch(
      userIntent.toString(),
      10
    );
    
    // Rank and filter by relevance
    const ranked = this.rankByRelevance(searchResults, currentFile);
    const selectedTier2 = this.selectWithinBudget(ranked, 4000);
    context.relevant = selectedTier2;
    budget.allocate(
      this.tokenCounter.count(selectedTier2),
      'relevant'
    );

    // 3. Retrieve Tier 3 (if budget remains)
    const remainingBudget = budget.remaining();
    if (remainingBudget > 200) {
      const archiveResults = await this.tier3.retrieveRelevantArchives(
        userIntent.toString(),
        3
      );
      const selectedTier3 = this.selectWithinBudget(archiveResults, remainingBudget);
      context.archive = selectedTier3;
      budget.allocate(
        this.tokenCounter.count(selectedTier3),
        'archive'
      );
    }

    // 4. Deduplication
    context.relevant = this.deduplicate(context.relevant);
    context.archive = this.deduplicate(context.archive);

    // 5. Finalize
    context.metadata.totalTokens = budget.used();
    
    return context;
  }

  private rankByRelevance(
    results: SearchResult[],
    currentFile: string
  ): SearchResult[] {
    return results.map(r => ({
      ...r,
      score: this.calculateRelevanceScore(r, currentFile)
    })).sort((a, b) => b.score - a.score);
  }

  private calculateRelevanceScore(
    result: SearchResult,
    currentFile: string
  ): number {
    let score = result.relevance;
    
    // Boost if in same directory
    if (path.dirname(result.file) === path.dirname(currentFile)) {
      score *= 1.5;
    }
    
    // Boost if recently modified
    const hoursSinceModified = (Date.now() - result.lastModified.getTime()) / 3600000;
    if (hoursSinceModified < 24) {
      score *= 1.3;
    }
    
    // Boost if imported by current file
    if (this.isImportedBy(result.file, currentFile)) {
      score *= 2.0;
    }
    
    return score;
  }

  private deduplicate<T extends { file: string }>(items: T[]): T[] {
    const seen = new Set<string>();
    return items.filter(item => {
      if (seen.has(item.file)) return false;
      seen.add(item.file);
      return true;
    });
  }
}

class TokenBudget {
  private total: number;
  private used: number = 0;
  private allocations: Map<string, number> = new Map();

  constructor(total: number) {
    this.total = total;
  }

  allocate(amount: number, category: string): boolean {
    if (this.used + amount > this.total) {
      return false;
    }
    this.used += amount;
    this.allocations.set(category, amount);
    return true;
  }

  remaining(): number {
    return this.total - this.used;
  }

  used(): number {
    return this.used;
  }
}
```

---

## Compression Strategies

### Smart Truncation

```typescript
class SmartCompressor {
  compressCode(content: string, maxTokens: number): string {
    const tokens = this.tokenize(content);
    
    if (tokens.length <= maxTokens) {
      return content;
    }

    // Strategy 1: Remove comments
    const withoutComments = this.removeComments(content);
    if (this.estimateTokens(withoutComments) <= maxTokens) {
      return withoutComments;
    }

    // Strategy 2: Extract signatures only
    const signatures = this.extractSignatures(content);
    if (this.estimateTokens(signatures) <= maxTokens) {
      return signatures;
    }

    // Strategy 3: AI summarization
    return this.summarizeWithAI(content, maxTokens);
  }

  private extractSignatures(code: string): string {
    // Parse AST and extract function/class signatures
    const ast = parse(code);
    return ast.body
      .filter(node => ['FunctionDeclaration', 'ClassDeclaration'].includes(node.type))
      .map(node => {
        if (node.type === 'FunctionDeclaration') {
          const params = node.params.map(p => p.name).join(', ');
          return `function ${node.id.name}(${params}): ${this.inferReturnType(node)};`;
        }
        return `class ${node.id.name} { ... };`;
      })
      .join('\n');
  }
}
```

### Semantic Compression

```typescript
class SemanticCompressor {
  async compress(
    content: string,
    context: string,
    maxTokens: number
  ): Promise<string> {
    // Use LLM to create semantic summary
    const prompt = `
      Compress this code to fit within ${maxTokens} tokens.
      Preserve all function signatures and key logic.
      Remove implementation details but keep the structure.
      
      Context: ${context}
      
      Code:
      ${content}
      
      Compressed version:
    `;

    const compressed = await this.llm.generate(prompt, {
      maxTokens: maxTokens,
      temperature: 0.1  // Low creativity for accuracy
    });

    return compressed;
  }
}
```

---

## Performance Characteristics

### Latency Budget

| Operation | Target | Max Acceptable |
|-----------|--------|----------------|
| Tier 1 Load | <10ms | 50ms |
| Tier 2 Search | <100ms | 500ms |
| Tier 3 Retrieval | <500ms | 2s |
| Context Assembly | <200ms | 1s |
| **Total** | **<810ms** | **3.5s** |

### Storage Requirements

| Tier | Storage per 1M LOC | Growth Rate |
|------|-------------------|-------------|
| Tier 1 | ~50KB | O(1) - constant |
| Tier 2 | ~500MB | O(n) - linear |
| Tier 3 | ~2GB | O(log n) - sub-linear |

### Token Efficiency

```
Traditional Approach:
  200K tokens/request × 100 requests/day = 20M tokens/day
  Cost: $12/day (Claude 3.5 Sonnet)

MemoryLayer Approach:
  6K tokens/request × 100 requests/day = 600K tokens/day
  Cost: $0.36/day

Savings: 97% cost reduction
```

---

## Implementation Notes

### Phase 1 (MVP) Simplifications

1. **Tier 1**: Full implementation (required)
2. **Tier 2**: Basic semantic search (sqlite-vss)
3. **Tier 3**: Simple file-based archive (no vector search yet)
4. **Compression**: Rule-based only (no AI compression yet)
5. **Pre-fetching**: Rule-based (no ML predictions yet)

### Phase 2+ Enhancements

1. **Tier 2**: Advanced embeddings, cross-file relationships
2. **Tier 3**: Vector search, intelligent consolidation
3. **Compression**: AI-powered semantic compression
4. **Pre-fetching**: ML-based predictive loading
5. **Optimization**: Learned token budgets per project

---

## Conclusion

The Hierarchical Memory System is the technical foundation that enables MemoryLayer's 100x value proposition. By implementing brain-inspired sparse storage and intelligent retrieval, we achieve:

- **Constant token usage** regardless of project size
- **Sub-second context assembly** for real-time coding
- **97% cost reduction** through smart compression
- **Improved quality** via focused, relevant context

This architecture scales from 10-line scripts to 10-million-line enterprise codebases without performance degradation or cost explosion.

---

## Next Steps

1. **Review the innovation**: See `documentation/04-core-innovations/01-sparse-hierarchical-memory.md`
2. **Validate economics**: See `documentation/06-business-model/02-unit-economics.md`
3. **Implementation plan**: See `documentation/05-implementation/01-phase-1-vscode-extension.md`

---

*Technical Specification v1.0 - MemoryLayer Team*
